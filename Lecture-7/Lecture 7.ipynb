{
 "metadata": {
  "name": "",
  "signature": "sha256:e59aa1ebbcc3f657a211ec4baf028a02fa6c899de8acf2306071e4f268a776c8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Introductory Exercises"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Work through the book\u2019s little spam filtering example (include in your notebook)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Creating classifier:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import math\n",
      "def getwords(doc):\n",
      "    splitter=re.compile('\\\\W*')\n",
      "    # Split the words by non-alpha characters\n",
      "    words=[s.lower( ) for s in splitter.split(doc)\n",
      "    if len(s)>2 and len(s)<20]\n",
      "    # Return the unique set of words only\n",
      "    return dict([(w,1) for w in words])\n",
      "\n",
      "class classifier:\n",
      "    def __init__(self,getfeatures,filename=None):\n",
      "        # Counts of feature/category combinations\n",
      "        self.fc={}\n",
      "        # Counts of documents in each category\n",
      "        self.cc={}\n",
      "        self.getfeatures=getfeatures\n",
      "        self.thresholds={}      \n",
      "    \n",
      "    def setthreshold(self,cat,t):\n",
      "        self.thresholds[cat]=t\n",
      "    \n",
      "    def getthreshold(self,cat):\n",
      "        if cat not in self.thresholds: return 1.0\n",
      "        return self.thresholds[cat]\n",
      "    \n",
      "    # Increase the count of a feature/category pair\n",
      "    def incf(self,f,cat):\n",
      "        self.fc.setdefault(f,{})\n",
      "        self.fc[f].setdefault(cat,0)\n",
      "        self.fc[f][cat]+=1\n",
      "    \n",
      "    # Increase the count of a category\n",
      "    def incc(self,cat):\n",
      "        self.cc.setdefault(cat,0)\n",
      "        self.cc[cat]+=1\n",
      "    \n",
      "    # The number of times a feature has appeared in a category\n",
      "    def fcount(self,f,cat):\n",
      "        if f in self.fc and cat in self.fc[f]:\n",
      "            return float(self.fc[f][cat])\n",
      "        return 0.0\n",
      "    \n",
      "    # The number of items in a category\n",
      "    def catcount(self,cat):\n",
      "        if cat in self.cc:\n",
      "            return float(self.cc[cat])\n",
      "        return 0\n",
      "    \n",
      "    # The total number of items\n",
      "    def totalcount(self):\n",
      "        return sum(self.cc.values( ))\n",
      "    \n",
      "    # The list of all categories\n",
      "    def categories(self):\n",
      "        return self.cc.keys( )\n",
      "    \n",
      "    def train(self,item,cat):\n",
      "        features=self.getfeatures(item)\n",
      "        # Increment the count for every feature with this category\n",
      "        for f in features:\n",
      "            self.incf(f,cat)\n",
      "            # Increment the count for this category\n",
      "        self.incc(cat)\n",
      "    \n",
      "    def fprob(self,f,cat):\n",
      "        if self.catcount(cat)==0: return 0\n",
      "        # The total number of times this feature appeared in this\n",
      "        # category divided by the total number of items in this category\n",
      "        return self.fcount(f,cat)/self.catcount(cat)\n",
      "    \n",
      "    def weightedprob(self,f,cat,prf,weight=1.0,ap=0.5):\n",
      "        # Calculate current probability\n",
      "        basicprob=prf(f,cat)\n",
      "        # Count the number of times this feature has appeared in\n",
      "        # all categories\n",
      "        totals=sum([self.fcount(f,c) for c in self.categories( )])\n",
      "        # Calculate the weighted average\n",
      "        bp=((weight*ap)+(totals*basicprob))/(weight+totals)\n",
      "        return bp\n",
      "   \n",
      "    def classify(self,item,default=None):\n",
      "        probs={}\n",
      "        # Find the category with the highest probability\n",
      "        max=0.0\n",
      "        for cat in self.categories( ):\n",
      "            probs[cat]=self.prob(item,cat)\n",
      "            if probs[cat]>max:\n",
      "                max=probs[cat]\n",
      "                best=cat\n",
      "        # Make sure the probability exceeds threshold*next best\n",
      "        for cat in probs:\n",
      "            if cat==best: continue\n",
      "            if probs[cat]*self.getthreshold(best)>probs[best]: return default\n",
      "        return best\n",
      "\n",
      "class naivebayes(classifier):\n",
      "    def docprob(self,item,cat):\n",
      "        features=self.getfeatures(item)\n",
      "        # Multiply the probabilities of all the features together\n",
      "        p=1\n",
      "        for f in features: p*=self.weightedprob(f,cat,self.fprob)\n",
      "        return p\n",
      "    def prob(self,item,cat):\n",
      "        catprob=self.catcount(cat)/self.totalcount( )\n",
      "        docprob=self.docprob(item,cat)\n",
      "        return docprob*catprob\n",
      "\n",
      "    \n",
      "def sampletrain(cl):\n",
      " cl.train('Nobody owns the water.','good')\n",
      " cl.train('the quick rabbit jumps fences','good')\n",
      " cl.train('buy pharmaceuticals now','bad')\n",
      " cl.train('make quick money at the online casino','bad')\n",
      " cl.train('the quick brown fox jumps','good')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we test the classifier:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classify = classifier(getwords)\n",
      "sampletrain(classify)\n",
      "print(classify.fcount(\"quick\",\"good\"))\n",
      "print(classify.fcount(\"quick\",\"bad\"))\n",
      "print(classify.fprob('quick','good'))\n",
      "print(classify.weightedprob('money','good',classify.fprob))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2.0\n",
        "1.0\n",
        "0.666666666667\n",
        "0.25\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and the naive bayes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bayes = naivebayes(getwords)\n",
      "sampletrain(bayes)\n",
      "print(bayes.prob('quick rabbit','good'))\n",
      "print(bayes.prob('quick rabbit','bad'))\n",
      "print(bayes.classify('quick rabbit',default='unknown'))\n",
      "print(bayes.classify('quick money',default='unknown'))\n",
      "bayes.setthreshold('bad',3.0)\n",
      "print(bayes.classify('quick money',default='unknown'))\n",
      "for i in range(10): \n",
      "    sampletrain(bayes)\n",
      "print(bayes.classify('quick money',default='unknown'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.15625\n",
        "0.05\n",
        "good\n",
        "bad\n",
        "unknown\n",
        "bad\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Explain what happens when you run sampletrain(cl) on p. 121? What function in sklearn.tree.DecisionTreeClassifier does this correspond to?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It trains the classifier based on the data given, thus it corresponds to the fit method of the DecisionTreeClassifier."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Express in your own words what a conditional probability is. In the spam-example, if a word has conditional probability = 0.5 of being \u201cgood\u201d, what does that mean about that word in the training data?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A conditional probability is the probability of an event ocurring given another event has occured.\n",
      "\n",
      "For example if Pr(saint|good) = 0.5 it means that there's a 50% probability a good document will contain the word \"saint\"."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####What\u2019s the \u201cAssumed probability\u201d on p. 122."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You set an assumed probability when you have little information about a feature (like when the classifier hasn't seen a lot of training data or with words which frequencies are very low).\n",
      "\n",
      "So setting an assumed probability of a word X to 0.5 and a weight of 1 and training the classifier with one bad document containing X, the probability of documents containing X will be 0.75 with the assumed probability instead of 1. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####How is the prior probability of a category (e.g. good/bad) defined?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The prior probability of for example bad is the number of instances which are bad divided by the total number of instances."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Explain Bayes\u2019 Theorem in your own words."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$\n",
      "\n",
      "P(A) and P(B) are the probabilities of event A and event B respectively, now P(B|A) is the probability of event B given event A, and P(B|A) is reversely the probability of event B given event A."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####In what sense is Naive Bayes \u201cnaive\u201d"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we can read from the book, Naive Bayes is naive in the sense that the probabilities are assumed to be independent.\n",
      "\n",
      "The example given in the book illustrates that a document category containing the word money probably have a higher probability of containing the word casino as well or other \"related words\" such as game or gambling, thus in the case of words the probabilities cannot be assumed to be independent."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Part A. Cross validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Exercise A1: Training error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Fit a prediction tree to X_BoW  using \u201cgood\u201d/\u201dbad\u201d as y."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Loading heroes and villains from text files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "\n",
      "with open(\"superheroes.txt\", \"r\") as f:\n",
      "    superhero_dictionary = json.load(f)\n",
      "with open(\"supervillains.txt\", \"r\") as f:\n",
      "    supervillain_dictionary = json.load(f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "create good/bad/neutral lists as well as prediction tree:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "good_heroes = [(key,value[\"revisions\"][0][\"*\"]) for key,value in superhero_dictionary.items() if key not in supervillain_dictionary]\n",
      "bad_heroes = [(key,value[\"revisions\"][0][\"*\"]) for key, value in supervillain_dictionary.items() if key not in superhero_dictionary]\n",
      "neutral_heroes = [(key,value[\"revisions\"][0][\"*\"]) for key, value in superhero_dictionary.items() if key in supervillain_dictionary]\n",
      "\n",
      "def is_page_acceptable(body):\n",
      "    return not body.startswith(\"#REDIRECT\")\n",
      "\n",
      "good_heroes = [hero for hero in good_heroes if is_page_acceptable(hero[1])]\n",
      "bad_heroes = [hero for hero in bad_heroes if is_page_acceptable(hero[1])]\n",
      "neutral_heroes = [hero for hero in neutral_heroes if is_page_acceptable(hero[1])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "creating ANEW dictionary:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "import pprint\n",
      "anew_dictionary = {}\n",
      "with open(\"Ratings_Warriner_et_al.csv\", \"rb\") as csvfile:\n",
      "    reader = csv.reader(csvfile, delimiter=\"\\n\")\n",
      "    for index, row in enumerate(reader):\n",
      "        splitted = row[0].split(\",\")\n",
      "        # print out header\n",
      "        if index == 0:\n",
      "            pprint.pprint(\",\".join([splitted[1], splitted[2], splitted[5], splitted[8]]))\n",
      "        else:\n",
      "            anew_dictionary[splitted[1]] = [float(splitted[2]), float(splitted[5]), float(splitted[8])]\n",
      "for key, val_list in anew_dictionary.items():\n",
      "    anew_dictionary[key] = [val-5 for val in val_list]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "'Word,V.Mean.Sum,A.Mean.Sum,D.Mean.Sum'\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Fit a prediction tree to X_BoW  using \u201cgood\u201d/\u201dbad\u201d as y"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn import tree\n",
      "import numpy as np\n",
      "# have to sort the dictionary words when giving them as we else cannot rely on index\n",
      "vectorizer = CountVectorizer(vocabulary=sorted(anew_dictionary.keys()))\n",
      "superhero_collection = [x[1] for x in good_heroes]+[x[1] for x in bad_heroes]\n",
      "\n",
      "X_bow = vectorizer.fit_transform(superhero_collection).toarray()\n",
      "y_bow = np.array([0 for x in good_heroes] + [1 for x in bad_heroes])\n",
      "C_bow = [\"Good\", \"Bad\"]\n",
      "M_bow = sorted(anew_dictionary.keys())\n",
      "\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "clf = clf.fit(X_bow, y_bow)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "creating X_anew:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calculate_scores(superheroes, bag_of_words, anew_weights):\n",
      "    superhero_scores = {}\n",
      "    valence_index = 0\n",
      "    arousal_index = 1\n",
      "    dominance_index = 2\n",
      "    anew_keys = sorted(anew_weights.keys())\n",
      "    for i, hero in enumerate(superheroes):\n",
      "        word_sum = 0\n",
      "        valence_score = 0\n",
      "        arousal_score = 0\n",
      "        dominance_score = 0\n",
      "        for j, word_count in enumerate(bag_of_words[i]):\n",
      "            dominance_score += word_count*anew_weights[anew_keys[j]][dominance_index]\n",
      "            arousal_score += word_count*anew_weights[anew_keys[j]][arousal_index]\n",
      "            valence_score += word_count*anew_weights[anew_keys[j]][valence_index]\n",
      "            word_sum += word_count\n",
      "        superhero_scores[hero] = [valence_score/word_sum, arousal_score/word_sum, dominance_score/word_sum]\n",
      "    return superhero_scores\n",
      "\n",
      "superhero_names = [x[0] for x in good_heroes]+[x[0] for x in bad_heroes]\n",
      "superhero_scores = calculate_scores(superhero_names, X_bow, anew_dictionary)\n",
      "X_anew = np.array([superhero_scores[x] for x in superhero_names])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Use the tree to predict classes of the data in X_BoW. The resulting prediction vector we call y_hat(as in \u0177 as \u2018^\u2019 usually denotes the estimated value)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_hat = clf.predict(X_bow)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Compare y_hat to y. How many incorrect predictions have we made?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "incorrect_predictions = np.sum(y_hat.ravel()!=y_bow.ravel(),dtype=int)\n",
      "print(incorrect_predictions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we can see from the output, the decision tree has made just a single incorrect prediction."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Calculate the ratio of incorrect predictions to the total number of predictions - the rate of errors. This measure we call the training error"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training_error = incorrect_predictions/float(len(y_hat))\n",
      "print(training_error)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.000725163161711\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we can see, the training error is very low."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Discuss the validity of this measure. Let\u2019s assume that we \u2018forgot\u2019 to include a handful of characters in our X. Would you expect this prediction tree to perform as well on these new characters? Please supply an argument to support your answer/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we get a very low training error of 0.0007 the measure is probably not valid since the Decision Tree is overfitted and not alleviated by pruning or restricting max depth.\n",
      "\n",
      "The classifier would not perform as well on unseen input like in the scenario of \"forgetting\" to include some characters."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Exercise A2: Test error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Use X_BoW as X and define T - the number of folds in for CV loop and Loop over each of the T subsets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can either roll our own solution or use the cross_validation library from sklearn.\n",
      "\n",
      "We define T = 5 for our cross validation loop.\n",
      "\n",
      "First our own way:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "N = len(X_bow)\n",
      "T = 5\n",
      "total_errors = 0\n",
      "\n",
      "def cross_validation_partition(X, y, K):\n",
      "    for k in xrange(K):\n",
      "        X_train = np.array([x for i, x in enumerate(X) if i % K != k])\n",
      "        X_test = np.array([x for i, x in enumerate(X) if i % K == k])\n",
      "        y_train = np.array([x for i, x in enumerate(y) if i % K != k])\n",
      "        y_test = np.array([x for i, x in enumerate(y) if i % K == k])\n",
      "        yield X_train, X_test, y_train, y_test\n",
      "\n",
      "for X_train, X_test, y_train, y_test in cross_validation_partition(X_bow, y_bow, T):\n",
      "    clf = tree.DecisionTreeClassifier()\n",
      "    clf = clf.fit(X_train, y_train)\n",
      "    y_hat = clf.predict(X_test)\n",
      "    incorrect_predictions = np.sum(y_hat.ravel()!=y_test.ravel(),dtype=int)\n",
      "    total_errors += incorrect_predictions\n",
      "    print(\"{} incorrect of {} predictions\".format(incorrect_predictions,len(y_test)))\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "75 incorrect of 276 predictions\n",
        "80 incorrect of 276 predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "71 incorrect of 276 predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "72 incorrect of 276 predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "79 incorrect of 275 predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Report the number of incorrectly classified observations across all T loops."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(total_errors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "377\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we can see we have a total of 394 mispredictions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Use this number to calculate an error rate as done in A1. This is the test error"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_error = total_errors/float(len(y_bow))\n",
      "print(test_error)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.273386511965\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now have an test error of 28.57%"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Compare this to the training error in A1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The error rate of 28.57% is much more reasonable than the error rate we got in A1 as this one is after doing cross validation and the A1 error rate was due to overfitting."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Part B. Parameter tuning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Obtaining a valid performance measure is crucial when choosing your model parameters.\n",
      "###Exercise B1: find the optimal min_sample_split using training error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Initialize n_ms instances of DecisionTreeClassifier each with different min_sample_split value. Use values ranging from 2^0  to 2^9 (see Exercise 6.B2)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_ms_values = [1,3,5,7,10,50,100,150,200,250,300,350,400,450,500]\n",
      "\n",
      "for n_ms_value in n_ms_values:\n",
      "    clf = tree.DecisionTreeClassifier(min_samples_split=n_ms_value)\n",
      "    clf = clf.fit(X_bow, y_bow)\n",
      "    y_hat = clf.predict(X_bow)\n",
      "    incorrect_predictions = np.sum(y_hat.ravel()!=y_bow.ravel(),dtype=int)\n",
      "    training_error = incorrect_predictions/float(len(y_hat))\n",
      "    print(\"training error is {} for i={}\".format(training_error,n_ms_value))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "training error is 0.000725163161711 for i=1\n",
        "training error is 0.00145032632342 for i=3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training error is 0.0181290790428 for i=5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training error is 0.0188542422045 for i=7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training error is 0.0261058738216 for i=10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training error is 0.0884699057288 for i=50"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training error is 0.123277737491 for i=100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training error is 0.171863669326 for i=150"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training error is 0.175489485134 for i=200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training error is 0.183466279913 for i=250"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training error is 0.204496011603 for i=300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training error is 0.232052211748 for i=350"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training error is 0.255257432922 for i=400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training error is 0.255257432922 for i=450"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training error is 0.255257432922 for i=500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see from the training errors outputted that it gets progressively worse with higher values of min_samples_split, until reaching 400 where we observe no difference with higher values."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Exercise B2: find the optimal min_sample_split\n",
      "####Initialize different models as in B1, For each model, repeat A2 and report test error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using value T = 5."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for n_ms_value in n_ms_values:\n",
      "    total_errors = 0\n",
      "    for X_train, X_test, y_train, y_test in cross_validation_partition(X_bow, y_bow, T):\n",
      "        clf = tree.DecisionTreeClassifier(min_samples_split=n_ms_value)\n",
      "        clf = clf.fit(X_train, y_train)\n",
      "        y_hat = clf.predict(X_test)\n",
      "        incorrect_predictions = np.sum(y_hat.ravel()!=y_test.ravel(),dtype=int)\n",
      "        total_errors += incorrect_predictions\n",
      "    test_error = total_errors/float(len(y_bow))\n",
      "    print(\"test error is {} for i={}\".format(test_error,n_ms_value))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test error is 0.281363306744 for i=1\n",
        "test error is 0.280638143582 for i=3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "test error is 0.287164612038 for i=5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "test error is 0.270485859318 for i=7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "test error is 0.298042059463 for i=10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "test error is 0.294416243655 for i=50"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "test error is 0.283538796229 for i=100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "test error is 0.279187817259 for i=150"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "test error is 0.274111675127 for i=200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "test error is 0.273386511965 for i=250"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "test error is 0.282088469906 for i=300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "test error is 0.274836838289 for i=350"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "test error is 0.274836838289 for i=400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "test error is 0.284989122553 for i=450"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "test error is 0.299492385787 for i=500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Comparing with the values in B1, we find the optimal min_samples_split value to be around 200-250, yielding the lowest test error of 27.33%."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Part C. Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Exercise C1: Gaussian vs Multinomial Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Explain the difference between the two models in your own words."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gaussian implements a Gaussian Naive Bayes algorithm where the likelihoods are assumed to be of Gaussian/Normal distribution typically continuous data.\n",
      "\n",
      "Multinomial implements an algorithm where the likelihoods are assumed to be multinomially distributed, such as data that can be turned into counts/frequencies, for example words in a document."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Calculate the test error for both models on the X_BoW data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculating test error for X_BoW with Gaussian Naive Bayes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.naive_bayes\n",
      "\n",
      "total_errors = 0\n",
      "for X_train, X_test, y_train, y_test in cross_validation_partition(X_bow, y_bow, T):\n",
      "    clf = sklearn.naive_bayes.GaussianNB()\n",
      "    clf.fit(X_train, y_train)\n",
      "    y_hat = clf.predict(X_test)\n",
      "    incorrect_predictions = np.sum(y_hat.ravel()!=y_test.ravel(),dtype=int)\n",
      "    total_errors += incorrect_predictions\n",
      "    print(\"{} incorrect of {} predictions\".format(incorrect_predictions,len(y_test)))\n",
      "test_error = total_errors/float(len(y_bow))\n",
      "print(test_error)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "78 incorrect of 276 predictions\n",
        "90 incorrect of 276 predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "94 incorrect of 276 predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "93 incorrect of 276 predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "92 incorrect of 275 predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.324147933285\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.naive_bayes\n",
      "\n",
      "total_errors = 0\n",
      "for X_train, X_test, y_train, y_test in cross_validation_partition(X_bow, y_bow, T):\n",
      "    clf = sklearn.naive_bayes.MultinomialNB()\n",
      "    clf.fit(X_train, y_train)\n",
      "    y_hat = clf.predict(X_test)\n",
      "    incorrect_predictions = np.sum(y_hat.ravel()!=y_test.ravel(),dtype=int)\n",
      "    total_errors += incorrect_predictions\n",
      "    print(\"{} incorrect of {} predictions\".format(incorrect_predictions,len(y_test)))\n",
      "test_error = total_errors/float(len(y_bow))\n",
      "print(test_error)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "53 incorrect of 276 predictions\n",
        "44 incorrect of 276 predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "51 incorrect of 276 predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "52 incorrect of 276 predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "63 incorrect of 275 predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.19071791153\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Calculate the test error for both models on the X_ANEW data.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculating test error for X_ANEW with Gaussian Naive Bayes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.naive_bayes\n",
      "\n",
      "total_errors = 0\n",
      "print(X_anew[0])\n",
      "for X_train, X_test, y_train, y_test in cross_validation_partition(X_anew, y_bow, T):\n",
      "    clf = sklearn.naive_bayes.GaussianNB()\n",
      "    clf.fit(X_train, y_train)\n",
      "    y_hat = clf.predict(X_test)\n",
      "    incorrect_predictions = np.sum(y_hat.ravel()!=y_test.ravel(),dtype=int)\n",
      "    total_errors += incorrect_predictions\n",
      "    print(\"{} incorrect of {} predictions\".format(incorrect_predictions,len(y_test)))\n",
      "test_error = total_errors/float(len(y_bow))\n",
      "print(test_error)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 5.61897494  4.25624146  5.35929385]\n",
        "85 incorrect of 276 predictions\n",
        "83 incorrect of 276 predictions\n",
        "93 incorrect of 276 predictions\n",
        "85 incorrect of 276 predictions\n",
        "80 incorrect of 275 predictions\n",
        "0.308919506889\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculating test error for X_ANEW with Multinomial Naive Bayes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.naive_bayes\n",
      "anew_dictionary_reverted = {}\n",
      "for key, val_list in anew_dictionary.items():\n",
      "    anew_dictionary_reverted[key] = [val+5 for val in val_list]\n",
      "\n",
      "superhero_scores = calculate_scores(superhero_names, X_bow, anew_dictionary_reverted)\n",
      "X_anew_reverted = np.array([superhero_scores[x] for x in superhero_names])\n",
      "\n",
      "total_errors = 0\n",
      "for X_train, X_test, y_train, y_test in cross_validation_partition(X_anew_reverted, y_bow, T):\n",
      "    clf = sklearn.naive_bayes.MultinomialNB()\n",
      "    clf.fit(X_train, y_train)\n",
      "    y_hat = clf.predict(X_test)\n",
      "    incorrect_predictions = np.sum(y_hat.ravel()!=y_test.ravel(),dtype=int)\n",
      "    total_errors += incorrect_predictions\n",
      "    print(\"{} incorrect of {} predictions\".format(incorrect_predictions,len(y_test)))\n",
      "test_error = total_errors/float(len(y_bow))\n",
      "print(test_error)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "119 incorrect of 276 predictions\n",
        "120 incorrect of 276 predictions\n",
        "120 incorrect of 276 predictions\n",
        "120 incorrect of 276 predictions\n",
        "119 incorrect of 275 predictions\n",
        "0.433647570703\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Which performs best on what kind of data?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the X_BoW data, the best performing model is the Multinomial Naive Bayes with a test error of 19.07% versus 32.41% of the Gaussian Naive Bayes model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Multinomial Bayes did not work initially on the X_anew as the range of the anew scores is [-4:4] and it doesn't work with negative values.\n",
      "\n",
      "When reverting the range back to the [1:9] range it achieved a test error of 43.36% in contrast to the 30.89% of the Gaussian (when using [-4:4]), so clearly in this case the Gaussian Naive Bayes is the better predicting model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}